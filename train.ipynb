{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxXiSpxEpjqT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3y_sCBbe6K_"
      },
      "source": [
        "!pip install numpy==1.19.4\r\n",
        "!pip install pandas==1.1.5\r\n",
        "!pip install tensorflow==2.4.0\r\n",
        "!pip install tensorflow-hub==0.10.0\r\n",
        "!pip install bert-for-tf2==0.14.7\r\n",
        "!pip install sentencepiece==0.1.94"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bDdpJiffGE9"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "try:\r\n",
        "    %tensorflow_version 2.x\r\n",
        "except Exception:\r\n",
        "    pass\r\n",
        "\r\n",
        "import bert\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdApjzVvfYiv"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\r\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NukUy5-fayK"
      },
      "source": [
        "dataset_path = r'/content/drive/MyDrive/Text-Classification/Datasets/cleaned_train.csv'\r\n",
        "data = pd.read_csv(dataset_path, dtype={'sentiment': int, 'text': str})\r\n",
        "data_labels = data.sentiment.values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3qzL6zcfzbU"
      },
      "source": [
        "# tokenizer.tokenize('The sunrise was beautiful this morning.')\r\n",
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('The sunrise was beautiful this morning.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5YfbzYjf8c9"
      },
      "source": [
        "def encode_sentence(sentence):\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\r\n",
        "\r\n",
        "inputs = data['text'].apply(encode_sentence)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf-e1QtTf_r2"
      },
      "source": [
        "random_data_with_len = [[sentence, data_labels[i], len(sentence)] for i, sentence in enumerate(inputs) if len(sentence) > 7]\r\n",
        "np.random.shuffle(random_data_with_len)\r\n",
        "random_data_with_len.sort(key=lambda x: x[2])\r\n",
        "sorted_data = [(sent[0], sent[1]) for sent in random_data_with_len]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwSzeiqGgBTs"
      },
      "source": [
        "dataset = tf.data.Dataset.from_generator(lambda: sorted_data, output_types=(tf.int32, tf.int32))\r\n",
        "# next(iter(dataset))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0LXa-2CgDOM"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "batched_dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\r\n",
        "# next(iter(batched_dataset))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Pt7jMJg3Bu"
      },
      "source": [
        "NB_BATCHES = np.ceil(len(sorted_data)/BATCH_SIZE)\r\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\r\n",
        "batched_dataset.shuffle(buffer_size=NB_BATCHES)\r\n",
        "train_dataset, test_dataset = batched_dataset.skip(NB_BATCHES_TEST), batched_dataset.take(NB_BATCHES_TEST)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrDRKfcrg3hV"
      },
      "source": [
        "class DCNN(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self, vocab_size, emb_dim=200, nb_filters=100, FFN_units=256, dropout_rate=0.2, training=False, name='dcnn'):\r\n",
        "        super(DCNN, self).__init__(name=name)\r\n",
        "        self.embedding = layers.Embedding(vocab_size, emb_dim)\r\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding='valid', activation='relu')\r\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding='valid', activation='relu')\r\n",
        "        self.quadgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding='valid', activation='relu')\r\n",
        "        self.pool = layers.GlobalMaxPooling1D()\r\n",
        "        self.dense1 = layers.Dense(units=FFN_units, activation='relu')\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "        self.densel = layers.Dense(units=1, activation='sigmoid') # nb_classes=2\r\n",
        "        # self.densel = layers.Dense(units=nb_classes, activation='softmax')\r\n",
        "\r\n",
        "    def call(self, inputs, training):\r\n",
        "        x = self.embedding(inputs)\r\n",
        "        x2 = self.bigram(x)\r\n",
        "        x2 = self.pool(x2)\r\n",
        "        x3 = self.trigram(x)\r\n",
        "        x3 = self.pool(x3)\r\n",
        "        x4 = self.quadgram(x)\r\n",
        "        x4 = self.pool(x4) # (batch_size, nb_filters)\r\n",
        "\r\n",
        "        merged = tf.concat([x2, x3, x4], axis=1) # (batch_size, 3*nb_filters)\r\n",
        "        merged = self.dense1(merged)\r\n",
        "        merged = self.dropout(merged, training)\r\n",
        "        output = self.densel(merged)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3V_-9DOg6kg"
      },
      "source": [
        "dcnn = DCNN(len(tokenizer.vocab))\r\n",
        "dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeQiyoj3g711"
      },
      "source": [
        "checkpoint = tf.train.Checkpoint(dcnn)\r\n",
        "cppath = '/content/drive/MyDrive/Text-Classification/Checkpoint/'\r\n",
        "cpmanager = tf.train.CheckpointManager(checkpoint, cppath, max_to_keep=1)\r\n",
        "\r\n",
        "if cpmanager.latest_checkpoint:\r\n",
        "    checkpoint.restore(cpmanager.latest_checkpoint)\r\n",
        "    print('latest checkpoint has been restored')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUAXMnONhD-U"
      },
      "source": [
        "class saveCheckpointCallback(tf.keras.callbacks.Callback):\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        cpmanager.save()\r\n",
        "        print('checkpoint has been created at {}'.format(cppath))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXx5ee0EhFnE"
      },
      "source": [
        "dcnn.fit(train_dataset, epochs=5, callbacks=[saveCheckpointCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVturMznhIhs"
      },
      "source": [
        "# results = dcnn.evaluate(test_dataset)\r\n",
        "# results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HsfuL1chvel"
      },
      "source": [
        "# dcnn.save('/content/drive/MyDrive/Text-Classification/model')\r\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/Text-Classification/model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}